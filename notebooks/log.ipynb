{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from utils import load_data, safe_indexing\n",
    "from utils import path_config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_disambiguation_scores = {}\n",
    "en_models  = ['en_multi_bert', 'en_bert', 'en_gpt']\n",
    "ru_models  = ['ru_multi_bert', 'ru_bert', 'ru_gpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_disambiguation(train):\n",
    "    label_to_id = train['label_to_id']\n",
    "    id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "    X = train['features']\n",
    "    y = [id_to_label[safe_indexing(y)] for y in train['labels']]\n",
    "    assert len(X) == len(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    train_x = scaler.fit_transform(X_train)\n",
    "    test_x = scaler.transform(X_test)\n",
    "\n",
    "    majority_baseline = DummyClassifier(strategy='most_frequent', random_state=0) \n",
    "    majority_baseline.fit(train_x, y_train)\n",
    "    y_pred = majority_baseline.predict(test_x)\n",
    "    majority_f1 = round(f1_score(y_test, y_pred, average=\"macro\", zero_division=0)*100, 2)\n",
    "    print(f'F1 score (majority baseline) = {majority_f1}')\n",
    "\n",
    "    random_baseline = DummyClassifier(strategy='uniform', random_state=0)\n",
    "    random_baseline.fit(train_x, y_train)\n",
    "    y_pred = random_baseline.predict(test_x)\n",
    "    random_f1 = round(f1_score(y_test, y_pred, average=\"macro\", zero_division=0)*100, 2)\n",
    "    print(f'F1 score (random baseline) = {random_f1}')\n",
    "\n",
    "    logreg = LogisticRegression(random_state=0, solver='saga')\n",
    "    logreg.fit(train_x, y_train)\n",
    "    y_pred=logreg.predict(test_x)\n",
    "    true_f1 = round(f1_score(y_test, y_pred, average=\"macro\", zero_division=0)*100, 2)\n",
    "    print(f'F1 score (logistic regression) = {true_f1}')\n",
    "    return {'majority': majority_f1, 'random': random_f1, 'f1': true_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en_multi_bert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (majority baseline) = 37.1\n",
      "F1 score (random baseline) = 49.59\n",
      "F1 score (logistic regression) = 88.41\n",
      "\n",
      "en_bert\n",
      "F1 score (majority baseline) = 37.16\n",
      "F1 score (random baseline) = 49.52\n",
      "F1 score (logistic regression) = 88.23\n",
      "\n",
      "en_gpt\n",
      "F1 score (majority baseline) = 37.28\n",
      "F1 score (random baseline) = 49.54\n",
      "F1 score (logistic regression) = 87.53\n"
     ]
    }
   ],
   "source": [
    "for model in en_models:\n",
    "    print(f'\\n{model}')\n",
    "    data = torch.load(config[model])\n",
    "    term_disambiguation_scores[model] = term_disambiguation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ru_multi_bert\n",
      "F1 score (majority baseline) = 42.12\n",
      "F1 score (random baseline) = 47.27\n",
      "F1 score (logistic regression) = 75.37\n",
      "\n",
      "ru_bert\n",
      "F1 score (majority baseline) = 42.25\n",
      "F1 score (random baseline) = 47.02\n",
      "F1 score (logistic regression) = 74.95\n",
      "\n",
      "ru_gpt\n",
      "F1 score (majority baseline) = 42.59\n",
      "F1 score (random baseline) = 46.8\n",
      "F1 score (logistic regression) = 73.43\n"
     ]
    }
   ],
   "source": [
    "for model in ru_models:\n",
    "    print(f'\\n{model}')\n",
    "    data = torch.load(config[model])\n",
    "    term_disambiguation_scores[model] = term_disambiguation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"term_disambiguation_scores.json\", \"w\") as fp:\n",
    "    json.dump(term_disambiguation_scores , fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_disambiguation_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theme_disambiguation(train, data_type, language='en'):\n",
    "    X, y = load_data(train, data_type, language)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    train_x = scaler.fit_transform(X_train)\n",
    "    test_x = scaler.transform(X_test)\n",
    "\n",
    "    majority_baseline = DummyClassifier(strategy='most_frequent', random_state=0) \n",
    "    majority_baseline.fit(train_x, y_train)\n",
    "    y_pred = majority_baseline.predict(test_x)\n",
    "    majority_f1 = round(f1_score(y_test, y_pred, average=\"macro\", zero_division=0)*100, 2)\n",
    "    print(f'F1 score (majority baseline) = {majority_f1}')\n",
    "\n",
    "    random_baseline = DummyClassifier(strategy='uniform', random_state=0)\n",
    "    random_baseline.fit(train_x, y_train)\n",
    "    y_pred = random_baseline.predict(test_x)\n",
    "    random_f1 = round(f1_score(y_test, y_pred, average=\"macro\", zero_division=0)*100, 2)\n",
    "    print(f'F1 score (random baseline) = {random_f1}')\n",
    "\n",
    "    logreg = LogisticRegression(random_state=0, solver='saga')\n",
    "    logreg.fit(train_x, y_train)\n",
    "    y_pred=logreg.predict(test_x)\n",
    "    true_f1 = round(f1_score(y_test, y_pred, average=\"macro\", zero_division=0)*100, 2) \n",
    "    print(f'F1 score (logistic regression) = {true_f1}')\n",
    "    return {'majority': majority_f1, 'random': random_f1, 'f1': true_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en_multi_bert (sentence based)\n",
      "F1 score (majority baseline) = 3.81\n",
      "F1 score (random baseline) = 11.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chistik/.local/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (logistic regression) = 52.48\n",
      "\n",
      "en_bert (sentence based)\n",
      "F1 score (majority baseline) = 3.81\n",
      "F1 score (random baseline) = 11.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chistik/.local/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (logistic regression) = 53.64\n",
      "\n",
      "en_gpt (sentence based)\n",
      "F1 score (majority baseline) = 3.81\n",
      "F1 score (random baseline) = 11.35\n",
      "F1 score (logistic regression) = 53.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chistik/.local/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for model in en_models:\n",
    "    print(f'\\n{model} (sentence based)')\n",
    "    data = torch.load(config[model])\n",
    "    theme_disambiguation_scores[model] = {} \n",
    "    theme_disambiguation_scores[model]['sentence'] = theme_disambiguation(data, data_type='sentence', language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ru_multi_bert (sentence based)\n",
      "F1 score (majority baseline) = 4.4\n",
      "F1 score (random baseline) = 9.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chistik/.local/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (logistic regression) = 35.85\n",
      "\n",
      "ru_bert (sentence based)\n",
      "F1 score (majority baseline) = 4.4\n",
      "F1 score (random baseline) = 9.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chistik/.local/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (logistic regression) = 33.02\n",
      "\n",
      "ru_gpt (sentence based)\n",
      "F1 score (majority baseline) = 4.4\n",
      "F1 score (random baseline) = 9.75\n",
      "F1 score (logistic regression) = 37.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chistik/.local/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for model in ru_models:\n",
    "    print(f'\\n{model} (sentence based)')\n",
    "    data = torch.load(config[model])\n",
    "    theme_disambiguation_scores[model] = {} \n",
    "    theme_disambiguation_scores[model]['sentence'] = theme_disambiguation(data, data_type='sentence', language='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "en_multi_bert (token based)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (majority baseline) = 3.91\n",
      "F1 score (random baseline) = 9.54\n",
      "F1 score (logistic regression) = 15.56\n",
      "\n",
      "en_bert (token based)\n",
      "F1 score (majority baseline) = 3.9\n",
      "F1 score (random baseline) = 9.25\n",
      "F1 score (logistic regression) = 16.02\n",
      "\n",
      "en_gpt (token based)\n",
      "F1 score (majority baseline) = 3.88\n",
      "F1 score (random baseline) = 9.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chistik/.local/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (logistic regression) = 24.8\n"
     ]
    }
   ],
   "source": [
    "for model in en_models:\n",
    "    print(f'\\n{model} (token based)')\n",
    "    data = torch.load(config[model])\n",
    "    theme_disambiguation_scores[model]['token'] = theme_disambiguation(data, data_type='token', language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ru_multi_bert (token based)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (majority baseline) = 3.59\n",
      "F1 score (random baseline) = 9.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chistik/.local/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (logistic regression) = 16.86\n",
      "\n",
      "ru_bert (token based)\n",
      "F1 score (majority baseline) = 3.64\n",
      "F1 score (random baseline) = 9.61\n",
      "F1 score (logistic regression) = 25.17\n",
      "\n",
      "ru_gpt (token based)\n",
      "F1 score (majority baseline) = 3.65\n",
      "F1 score (random baseline) = 9.36\n",
      "F1 score (logistic regression) = 25.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chistik/.local/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for model in ru_models:\n",
    "    print(f'\\n{model} (token based)')\n",
    "    data = torch.load(config[model])\n",
    "    theme_disambiguation_scores[model]['token'] = theme_disambiguation(data, data_type='token', language='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"theme_disambiguation_scores.json\", \"w\") as fp:\n",
    "    json.dump(theme_disambiguation_scores , fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

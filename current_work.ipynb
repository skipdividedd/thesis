{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/chistik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    set_seed, \n",
    ")\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from plotly import graph_objects as go\n",
    "from collections import Counter\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "set_seed(41)\n",
    "cache_dir=\"./cache\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(model_name):\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        cache_dir=cache_dir,\n",
    "        torch_dtype=config.torch_dtype).to(device)\n",
    "    \n",
    "    if config.model_type in {\"gpt2\", \"roberta\"}:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir, use_fast=True, add_prefix_space=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir, use_fast=True)\n",
    "        \n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.eos_token:\n",
    "            tokenizer.pad_token = tokenizer.eos_token  \n",
    "            print(f'Set pad_token to be equal to eos_token: {tokenizer.pad_token}')\n",
    "        else:\n",
    "            raise ValueError(\"The tokenizer does not have an eos_token set.\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    text = row['text']['en'].lower()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    terms_with_counts = row['terms']\n",
    "    flattened_terms = list(itertools.chain.from_iterable([[term] * count for term_dict in terms_with_counts for term in term_dict['term'].split('_') for count in [term_dict['count']]]))\n",
    "    number_of_terms = len(flattened_terms)\n",
    "\n",
    "    labels = []\n",
    "    for word in lemmatized_tokens:\n",
    "        if word in flattened_terms:\n",
    "            labels.append('term')\n",
    "            flattened_terms.remove(word)\n",
    "        else:\n",
    "            labels.append('not_term')\n",
    "    assert len(tokens) == len(labels)        \n",
    "    if number_of_terms > labels.count('term'):\n",
    "        if number_of_terms - labels.count('term') > 5:\n",
    "            return np.nan, np.nan\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list=['term', 'not_term']\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\" \n",
    "max_length=512\n",
    "def tokenize_and_align_labels(examples, label_all_tokens=False):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['text'],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['terms']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                if label_all_tokens:\n",
    "                    label_ids.append(label_to_id[label[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google-bert/bert-base-uncased'\n",
    "model, tokenizer = get_model_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='eng_texts_terms_science.jsonl' \n",
    "df = pd.read_json(path_or_buf=file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'text': [],\n",
    "    'terms': []\n",
    "}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tokens, labels = process_row(row)\n",
    "    data['text'].append(tokens)\n",
    "    data['terms'].append(labels)\n",
    "\n",
    "new_df = pd.DataFrame(data)\n",
    "new_df.dropna(inplace=True)\n",
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99267, 12930)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = [token for sublist in new_df['text'] for token in sublist]\n",
    "len(all_tokens), len(set(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_words_labels(texts, labels):\n",
    "    return [(word, label)\n",
    "            for tokens, labels in zip(texts, labels)\n",
    "            for word, label in zip(tokens, labels)]\n",
    "\n",
    "all_words_labels = flatten_words_labels(new_df['text'], new_df['terms'])\n",
    "words, labels = zip(*all_words_labels)\n",
    "word_freq = Counter(words)\n",
    "sorted_words = sorted(word_freq, key=word_freq.get, reverse=True) # sort by frequency\n",
    "sorted_freqs = [word_freq[word] for word in sorted_words]\n",
    "word_to_label = dict(all_words_labels)\n",
    "sorted_labels = [word_to_label[word] for word in sorted_words]\n",
    "df_plot = pd.DataFrame({\n",
    "    'Word': sorted_words,\n",
    "    'Frequency': sorted_freqs,\n",
    "    'Rank': range(1, len(sorted_freqs) + 1),\n",
    "    'Label': sorted_labels\n",
    "})\n",
    "# expected frequency of each word under Zipfian distribution\n",
    "df_plot['Zipf_Frequency'] = df_plot['Frequency'][0] / df_plot['Rank']\n",
    "\n",
    "color_mapping = {'term': 'blue', 'not_term': 'green'}\n",
    "df_plot['Color'] = df_plot['Label'].map(color_mapping)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_plot['Rank'], y=df_plot['Frequency'], mode='markers',\n",
    "    marker=dict(color=df_plot['Color'], opacity=0.4), text=df_plot['Word'],\n",
    "    name='Frequency'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_plot['Rank'], y=df_plot['Zipf_Frequency'], mode='lines',\n",
    "    line=dict(color='orange', width=2), name=\"Zipf's Law\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Zipf's Law Visualization\", title_x=0.5,\n",
    "    xaxis=dict(title='Rank of word (log scale)', type='log'),\n",
    "    yaxis=dict(title='Frequency of word (log scale)', type='log'),\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "def calculate_zipf_metrics(df): \n",
    "    log_ranks = np.log(df['Rank']) \n",
    "    log_freqs = np.log(df['Frequency']) \n",
    "    slope, intercept, r_value, p_value, std_err = linregress(log_ranks, log_freqs) \n",
    "    return slope, intercept, r_value, p_value, std_err \n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = calculate_zipf_metrics(df_plot)\n",
    "# value close to -1 -> dataset follows Zipf's law; ideal slope would be -1\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "# r-value close to -1 suggests a strong negative linear relationship\n",
    "print(f\"Correlation coefficient (r): {r_value}\")\n",
    "# null hypothesis that the slope is zero (no relationship)\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'terms', '__index_level_0__'],\n",
       "    num_rows: 680\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(new_df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc1bafd096b451ebb84ede30685d1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = dataset.map(\n",
    "                tokenize_and_align_labels,\n",
    "                batched=True,\n",
    "                desc=\"Running tokenizer on train dataset\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'terms', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "class CustomDataset(Dataset):\n",
    "        def __init__(self, input_ids, attention_masks, labels):\n",
    "            self.input_ids = input_ids\n",
    "            self.attention_masks = attention_masks\n",
    "            self.labels = labels\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return {\n",
    "                'input_ids': self.input_ids[idx],\n",
    "                'attention_mask': self.attention_masks[idx],\n",
    "                'labels': self.labels[idx]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_representations(data, model, model_type='encoder'):\n",
    "\n",
    "    input_ids = [torch.tensor(d['input_ids']).unsqueeze(0) for d in data]\n",
    "    attention_masks = [torch.tensor(d['attention_mask']).unsqueeze(0) for d in data]\n",
    "    labels = [torch.tensor(d['labels']).unsqueeze(0) for d in data]  \n",
    "        \n",
    "    custom_dataset = CustomDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "    weighted_embeddings = []\n",
    "    mean_embeddings = []\n",
    "    embeddings = []\n",
    "    all_hidden_states = []\n",
    "    label_list = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():  \n",
    "        for sequence in tqdm(custom_dataset, desc=\"Processing\"):\n",
    "\n",
    "            input_ids = sequence['input_ids'].to(device)\n",
    "            attention_mask = sequence['attention_mask'].to(device)\n",
    "           \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            hidden_states = outputs[0]\n",
    "        \n",
    "            if model_type != 'encoder':\n",
    "                sequence_lengths = (torch.ne(input_ids, tokenizer.pad_token_id).sum(-1) - 1).to(device)\n",
    "                # embedding from last token\n",
    "                pooled_hidden_states = hidden_states[torch.arange(1, device=device), sequence_lengths]\n",
    "                # Weighted-Mean-Pooling\n",
    "                # https://stackoverflow.com/questions/76926025/sentence-embeddings-from-llama-2-huggingface-opensource\n",
    "                cumsum = attention_mask.cumsum(dim=1)\n",
    "                weights_for_non_padding = torch.where(attention_mask == 1, cumsum, attention_mask)\n",
    "                sums = torch.sum(weights_for_non_padding, dim=-1).unsqueeze(-1)\n",
    "                normalized_weights = weights_for_non_padding / sums # normalize so that weights sum to 1\n",
    "                sentence_embedding = torch.sum(hidden_states * normalized_weights.unsqueeze(-1), dim=1)\n",
    "                weighted_embeddings.append(sentence_embedding.cpu())\n",
    "            else:\n",
    "                pooled_hidden_states = outputs.last_hidden_state[:,0,:] # cls for bert\n",
    "\n",
    "            # normal average\n",
    "            num_tokens = torch.sum(attention_mask, dim=-1).unsqueeze(-1)\n",
    "            mean_embedding = torch.sum(hidden_states * attention_mask.unsqueeze(-1), dim=1)\n",
    "            mean_embedding = mean_embedding / num_tokens\n",
    "            mean_embeddings.append(mean_embedding.cpu())\n",
    "\n",
    "            embeddings.append(pooled_hidden_states.cpu())\n",
    "            label_list.append(sequence['labels'])\n",
    "            all_hidden_states.append(hidden_states.cpu())\n",
    "\n",
    "    final_last_embeddings = torch.cat(embeddings, dim=0)\n",
    "    final_mean_embeddings = torch.cat(mean_embeddings, dim=0)\n",
    "    all_hidden_states = torch.cat(all_hidden_states, dim=0)\n",
    "    label_list = torch.cat(label_list, dim=0)\n",
    "    \n",
    "    if model_type != 'encoder':\n",
    "        final_weighted_embeddings = torch.cat(weighted_embeddings, dim=0)\n",
    "    return all_hidden_states, final_last_embeddings, final_mean_embeddings, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 680/680 [00:09<00:00, 70.58it/s]\n"
     ]
    }
   ],
   "source": [
    "all_hidden_states, final_last_embeddings, final_mean_embeddings, label_list = extract_representations(train_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert final_last_embeddings.shape == final_mean_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([680, 512, 768])\n",
      "torch.Size([680, 512])\n"
     ]
    }
   ],
   "source": [
    "print(all_hidden_states.shape)\n",
    "print(label_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word_type='term', sentence_representation=final_last_embeddings):\n",
    "    cosine_similarities = []\n",
    "    num_terms = 0\n",
    "    num_examples, max_tokens, emb_dim = all_hidden_states.size()\n",
    "    for i in range(num_examples):\n",
    "        term_mask = label_list[i] == label_to_id[word_type]  # a boolean mask\n",
    "        term_embeddings = all_hidden_states[i][term_mask]\n",
    "        num_terms += term_embeddings.shape[0]\n",
    "        final_embedding = sentence_representation[i].view(1, -1)\n",
    "        for token_embedding in term_embeddings:\n",
    "            token_embedding = token_embedding.view(1, -1) \n",
    "            similarity = F.cosine_similarity(token_embedding, final_embedding)\n",
    "            cosine_similarities.append(similarity.item())  \n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18263093057169646"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities = similarity()\n",
    "np.average(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19237815565088573"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities = similarity(word_type='not_term')\n",
    "np.average(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5574284500752051"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities = similarity(sentence_representation=final_mean_embeddings)\n",
    "np.average(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5841845358850314"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities = similarity(word_type='not_term', sentence_representation=final_mean_embeddings)\n",
    "np.average(cosine_similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
